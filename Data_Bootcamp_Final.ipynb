{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Imports and Installs\n",
        "\n",
        "# Praw Install\n",
        "!pip install praw\n",
        "\n",
        "# General Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import yfinance as yf\n",
        "\n",
        "# Classification Model Imports\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, precision_score, fbeta_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Neural Net Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Sentiment Analyzer Imports\n",
        "import praw\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "5D16S4QH29D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Commodities\n",
        "Because we are analyzing the price changes of different commodities, we are using ETFs for price data. These values represent either the spot price (how much to buy the commodity now) or the futures price (how much to buy the commodity in the future). We use the following 9 ETFs, spanning across precious metals, fuels, and agriculture.\n",
        "\n",
        "\n",
        "*   GLD = SPDR Gold Shares (Spot Price)\n",
        "*   SLV\t= iShares Silver Trust (Spot Price)\n",
        "*   PPLT = abrdn Physical Platinum Shares ETF (Spot Price)\n",
        "*   CPER = United States Copper Index Fund (Futures)\n",
        "*   USO = United States Oil Fund (Futures)\n",
        "*   UNG = United States Natural Gas Fund (Futures)\n",
        "*   WEAT = Teucrium Wheat Fund (Futures)\n",
        "*   SOYB = Teucrium Soybean Fund (Futures)\n",
        "*   CORN = Teucrium Corn Fund (Futures)\n",
        "\n",
        "\n",
        "The ETF we are interested in classifying is VGT, Vanguard's Information Technology Index Fund.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gEXP1Pwomqy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tickers of commodities\n",
        "commodities = ['GLD', 'SLV', 'PPLT', 'CPER', 'USO', 'UNG', 'WEAT', 'SOYB', 'CORN', 'VGT']"
      ],
      "metadata": {
        "id": "u-ccwyFdms0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pulling YFinance Data\n",
        "Using the yfinance API, we can access information about any stock. Given this data, we pull closing prices to make our analysis. All of this information is stored in commodity_prices_data to be transformed into a data frame for convenience."
      ],
      "metadata": {
        "id": "Uelsq3NWmvLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initalize a list to keep data\n",
        "commodity_prices_data = []\n",
        "\n",
        "# Attempt to pull data and provide an error code\n",
        "for commodity in commodities:\n",
        "  try:\n",
        "    # Pull historical prices\n",
        "    commodity_prices = yf.Ticker(commodity).history(period = \"10y\")['Close']\n",
        "    commodity_prices.rename(commodity, inplace=True)\n",
        "    print(f'{commodity} data fetch successful')\n",
        "\n",
        "    # Append data to list\n",
        "    commodity_prices_data.append(commodity_prices)\n",
        "  except Exception as e:\n",
        "    print(f'Error: exception {e}')"
      ],
      "metadata": {
        "id": "Wf2bkagG20Ts",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to DataFrame\n",
        "commodity_df = pd.concat(commodity_prices_data, axis=1)\n",
        "commodity_df"
      ],
      "metadata": {
        "id": "O60qjPU325YE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with the Data\n",
        "With this data, we can plot the commodity prices over time. However, we are interested in how the change in commodity prices affect VGT's direction of movement. So, we calculate and plot percent change. Additionally, we will shift the VGT percent change such that the commodity changes for a specific trading day will be aligned with the VGT change in the next day."
      ],
      "metadata": {
        "id": "qc1pHG83m4pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "ax = sns.lineplot(data = commodity_df)\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Price')\n",
        "ax.set_title('Prices in the Last 10 Years');"
      ],
      "metadata": {
        "id": "_jcjyTSo3Nd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize percent change\n",
        "commodity_pct_change = commodity_df.pct_change()\n",
        "commodity_pct_change.dropna(inplace=True)\n",
        "commodity_pct_change"
      ],
      "metadata": {
        "id": "bC6Rq7bWmYtC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot each commodity compared to VGT\n",
        "fig, ax = plt.subplots(3, 3, figsize = (25, 25))\n",
        "\n",
        "# Flatten axes for ease\n",
        "ax = ax.flatten()\n",
        "\n",
        "# Iterate over each commodity\n",
        "for (index, commodity) in enumerate(commodities[:-1]):\n",
        "  sns.lineplot(data = commodity_pct_change[['VGT', commodity]], ax = ax[index])\n",
        "  ax[index].set_xlabel('Date')\n",
        "  ax[index].tick_params(axis = 'x', rotation = 45)\n",
        "  ax[index].set_ylabel(f'{commodity} Percent Change')\n",
        "  ax[index].set_title(f'{commodity} compared to VGT Percent Changes in the Last 10 Years');"
      ],
      "metadata": {
        "id": "_7CLIviIm9O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Classification Models\n",
        "Now, we define our features as the commodity percent changes. Because the goal is to predict VGT's direction of movement, we represent any positive percent change as a 1 and represent any negative (or zero) percent change as 0. This will be our target column.. This will be our target column.\n",
        "\n",
        "We will use the following models:\n",
        "\n",
        "*   Logistic Regression\n",
        "*   K-Neighbors Classification\n",
        "*   XGB Classification\n",
        "\n",
        "To evaluate our models, we can use precision or an F-beta score. Assuming this model only applies to long VGT, it is bad to predict VGT goes up when it actually goes down (incurring losses), but not as bad to predict VGT goes down when it actually goes up (missing gains). If the investor wants to avoid losses, precision is a good evaluator. However, if the investor avoids all losses, they would never make any money. The F-beta score provides a metric that is a mix of both precision and recall, balancing loss aversion and gain aversion. We will use a beta of 0.5 to prioritize precision."
      ],
      "metadata": {
        "id": "R8nc5M8PnEfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features\n",
        "X = commodity_pct_change.drop(columns = ['VGT'])\n",
        "\n",
        "# Define target where positive pct_change is classified as up (1) and negative or 0 pct_change is classified as down (0)\n",
        "y = np.where((commodity_pct_change['VGT'] > 0), 1, 0)\n",
        "\n",
        "# Perform train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 12)"
      ],
      "metadata": {
        "id": "IPG0GPS2nGQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data transformer\n",
        "transformer = StandardScaler()"
      ],
      "metadata": {
        "id": "KMjDsPacq60c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Baseline\n",
        "# Determine baseline\n",
        "days_up = pd.Series(y).value_counts()[1]\n",
        "days_down = pd.Series(y).value_counts()[0]\n",
        "print(f'Days up: {days_up}')\n",
        "print(f'Days down: {days_down}')\n",
        "print(f'Percent of days up{100 * (days_up / (days_up + days_down)): .2f}%')\n",
        "print(f'Percent of days down{100 * (days_down / (days_up + days_down)): .2f}%')"
      ],
      "metadata": {
        "id": "93tcRJKNq6V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scores dataframe to add model scores to\n",
        "\n",
        "scores_df = scores_df = pd.DataFrame(columns=[\n",
        "    'Model',\n",
        "    'Precision Score',\n",
        "    'F-Beta Score'\n",
        "])"
      ],
      "metadata": {
        "id": "8TNTGyXHvXIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Logistic Regression Model\n",
        "\n",
        "# Instantiate\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Create a pipe\n",
        "lr_pipe = Pipeline([('transformer', transformer), ('model', lr)])\n",
        "\n",
        "# Fit\n",
        "lr_pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "BBhoSeWqq75S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalute model\n",
        "ConfusionMatrixDisplay.from_estimator(lr_pipe, X_test, y_test, display_labels = ['Down', 'Up'])\n",
        "plt.title('LR Testing Performance');"
      ],
      "metadata": {
        "id": "aBZInrOgq80W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision Score\n",
        "precision_score(y_test, lr_pipe.predict(X_test))"
      ],
      "metadata": {
        "id": "76JF0Wu7q9yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# F-Beta Score\n",
        "fbeta_score(y_test, lr_pipe.predict(X_test), beta = 0.5)"
      ],
      "metadata": {
        "id": "fs9zPHnIq-pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding scores to dataframe\n",
        "\n",
        "scores_df.loc[len(scores_df)] = {\n",
        "    'Model': 'Logistic Regressor',\n",
        "    'Precision Score': precision_score(y_test, lr_pipe.predict(X_test)),\n",
        "    'F-Beta Score': fbeta_score(y_test, lr_pipe.predict(X_test), beta = 0.5)\n",
        "}"
      ],
      "metadata": {
        "id": "fq5YxMQrmpVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title K-Nearest Neighbors Classification Model\n",
        "\n",
        "# Instantiate\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Create a pipe\n",
        "knn_pipe = Pipeline([('transformer', transformer), ('model', knn)])\n",
        "\n",
        "# Fit\n",
        "knn_pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "1ejKlE_wq_WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "knn_params = {'model__n_neighbors': np.arange(1, 30, 2)}\n",
        "\n",
        "# Initialize a grid search\n",
        "knn_grid_search = GridSearchCV(estimator = knn_pipe, param_grid = knn_params, scoring = 'precision')\n",
        "\n",
        "# Fit grid search\n",
        "knn_grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "KmXTLGPZrAcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize performance of the model\n",
        "ConfusionMatrixDisplay.from_estimator(knn_grid_search, X_test, y_test, display_labels = ['Down', 'Up'])\n",
        "plt.title('KNN Testing Performance');"
      ],
      "metadata": {
        "id": "DZ29dCtmrCHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print best param\n",
        "print(f'Optimal Number of Neighbors: {knn_grid_search.best_params_}')\n",
        "\n",
        "# Print best precision score\n",
        "print(f'Best Precision Score: {precision_score(y_test, knn_grid_search.predict(X_test))}')\n",
        "\n",
        "# Print best F-beta score\n",
        "print(f'Best F-beta Score: {fbeta_score(y_test, knn_grid_search.predict(X_test), beta = 0.5)}')"
      ],
      "metadata": {
        "id": "D6fPDgjHrC9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding scores to dataframe\n",
        "\n",
        "scores_df.loc[len(scores_df)] = {\n",
        "    'Model': 'K-Nearest Neighbours',\n",
        "    'Precision Score': precision_score(y_test, knn_pipe.predict(X_test)),\n",
        "    'F-Beta Score': fbeta_score(y_test, knn_pipe.predict(X_test), beta = 0.5)\n",
        "}"
      ],
      "metadata": {
        "id": "ykH6Tsa2m_2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title XGB Classification Model\n",
        "\n",
        "# Instantiate\n",
        "xgbc = XGBClassifier()\n",
        "\n",
        "# Create a pipe\n",
        "xgbc_pipe = Pipeline([('transformer', transformer), ('model', xgbc)])\n",
        "\n",
        "# Fit\n",
        "xgbc_pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "YjSrxk_UrEIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "xgbc_params = {'model__max_depth': np.arange(1, 30, 2)}\n",
        "\n",
        "# Initialize a grid search\n",
        "xgbc_grid_search = GridSearchCV(estimator = xgbc_pipe, param_grid = xgbc_params, scoring = 'precision')\n",
        "\n",
        "# Fit grid search\n",
        "xgbc_grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "6enhRF3trFDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalute model\n",
        "ConfusionMatrixDisplay.from_estimator(xgbc_grid_search, X_test, y_test, display_labels = ['Down', 'Up'])\n",
        "plt.title('XGBC Testing Performance');"
      ],
      "metadata": {
        "id": "nq35GQswrGFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print best param\n",
        "print(f'Optimal Max Depth: {xgbc_grid_search.best_params_}')\n",
        "\n",
        "# Print best precision score\n",
        "print(f'Best Precision Score: {precision_score(y_test, xgbc_grid_search.predict(X_test))}')\n",
        "\n",
        "# Print best F-beta score\n",
        "print(f'Best F-beta Score: {fbeta_score(y_test, xgbc_grid_search.predict(X_test), beta = 0.5)}')"
      ],
      "metadata": {
        "id": "lWYAKFbmrHES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding scores to dataframe\n",
        "\n",
        "scores_df.loc[len(scores_df)] = {\n",
        "    'Model': 'XGB Classifier',\n",
        "    'Precision Score': precision_score(y_test, xgbc_pipe.predict(X_test)),\n",
        "    'F-Beta Score': fbeta_score(y_test, xgbc_pipe.predict(X_test), beta = 0.5)\n",
        "}"
      ],
      "metadata": {
        "id": "J4rfduSPnIfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Net Model"
      ],
      "metadata": {
        "id": "drWw1mDFuCrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert training and test features to PyTorch tensors\n",
        "X_train_torch = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "X_test_torch  = torch.tensor(X_test.values,  dtype=torch.float32)\n",
        "\n",
        "# Convert labels to tensors and reshape to (N, 1)\n",
        "y_train_torch = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "y_test_torch  = torch.tensor(y_test,  dtype=torch.float32).unsqueeze(1)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "07ES8cUIuCrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Model\n",
        "model = nn.Sequential(nn.Linear(9,128), # Input/first hidden layer, 9 inputs, 128 neurons\n",
        "                       nn.ReLU(), # Activation layer\n",
        "                       nn.Linear(128,64), # Second hidden layer, 128 inputs, 64 neurons\n",
        "                       nn.ReLU(),\n",
        "                       nn.Linear(64,32), # Third hidden layer, 64 inputs, 32 neurons\n",
        "                       nn.ReLU(),\n",
        "                       nn.Linear(32,1)) # Output layer, 32 inputs, 1 output\n"
      ],
      "metadata": {
        "id": "WC6AYznGuCrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty list to append losses to\n",
        "losses = []\n",
        "\n",
        "# Loss function\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Optimizer to update weights\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop for 50 epochs\n",
        "for i in range(50):\n",
        "    yhat = model(X_train_torch) # Model outputs\n",
        "    loss = loss_fn(yhat, y_train_torch) # Compute loss\n",
        "    optimizer.zero_grad() # Clear old gradient\n",
        "    loss.backward() # Compute gradient\n",
        "    optimizer.step() # Update weights\n",
        "    losses.append(loss.item()) # Store loss"
      ],
      "metadata": {
        "id": "lMvXysNSuCrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Neural Net Loss Graph\n",
        "\n",
        "plt.plot(losses, '--o')\n",
        "plt.title(\"Neural Net losses function\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")"
      ],
      "metadata": {
        "id": "6eE48Gb3uCrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switching model to evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(X_test_torch) # Model outputs\n",
        "    probs = torch.sigmoid(logits) # Converting logits to probabilites\n",
        "    y_pred = (probs > 0.5).int() # Outputs with probability > 0.5 assigned 1, else 0\n",
        "\n",
        "# True values for evaluation\n",
        "y_true = y_test_torch.int()"
      ],
      "metadata": {
        "id": "knhEZOpGuCru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Precision score:{precision_score(y_true.numpy(), y_pred.numpy())}')"
      ],
      "metadata": {
        "id": "iadfws0AuCru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'F-Beta score:{fbeta_score(y_true.numpy(), y_pred.numpy(), beta=0.5)}')"
      ],
      "metadata": {
        "id": "4U0g-SrXuCru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_df.loc[len(scores_df)] = {\n",
        "    'Model': 'Neural Net',\n",
        "    'Precision Score': precision_score(y_true.numpy(), y_pred.numpy()),\n",
        "    'F-Beta Score': fbeta_score(y_true.numpy(), y_pred.numpy(), beta=0.5)\n",
        "}"
      ],
      "metadata": {
        "id": "x3G0y0XEuCru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Determining Feature Importances\n",
        "At this point, we compare the important features linked to changes in VGT price across the 3 models."
      ],
      "metadata": {
        "id": "YugQkjW-rJec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Best Performing - XGB Classification Feature Importances\n",
        "\n",
        "# Show feature importance\n",
        "xgbc_importances = permutation_importance(xgbc_grid_search, X_test, y_test, random_state=12)\n",
        "\n",
        "# Create a data frame to neatly organize\n",
        "xgbc_importances_df = pd.DataFrame()\n",
        "xgbc_importances_df['Features'] = X.columns\n",
        "xgbc_importances_df['Mean Importance'] = xgbc_importances['importances_mean']\n",
        "xgbc_importances_df['STD Importance'] = xgbc_importances['importances_std']\n",
        "\n",
        "# Reorder based on the mean importance\n",
        "xgbc_importances_df.sort_values(by='Mean Importance', inplace=True, ascending=False)\n",
        "\n",
        "# Plot importances\n",
        "sns.barplot(data = xgbc_importances_df, x = 'Mean Importance', y = 'Features')\n",
        "plt.axvline(0, color = 'black', linestyle = '--')\n",
        "plt.title('XGBC | Mean Importances per Features');"
      ],
      "metadata": {
        "id": "tZ-iCsOjq1VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Moderate Performing - Logistic Regression Feature Importances\n",
        "\n",
        "# Show feature importance\n",
        "lr_importances = permutation_importance(lr_pipe, X_test, y_test, random_state=12)\n",
        "\n",
        "# Create a data frame to neatly organize\n",
        "lr_importances_df = pd.DataFrame()\n",
        "lr_importances_df['Features'] = X.columns\n",
        "lr_importances_df['Mean Importance'] = lr_importances['importances_mean']\n",
        "lr_importances_df['STD Importance'] = lr_importances['importances_std']\n",
        "\n",
        "# Reorder based on the mean importance\n",
        "lr_importances_df.sort_values(by='Mean Importance', inplace=True, ascending=False)\n",
        "\n",
        "# Plot importances\n",
        "sns.barplot(data = lr_importances_df, x = 'Mean Importance', y = 'Features')\n",
        "plt.axvline(0, color = 'black', linestyle = '--')\n",
        "plt.title('LR | Mean Importances per Features');"
      ],
      "metadata": {
        "id": "LVUvEHWprWEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Worst Performing - K-Nearest Neighbors Classification Feature Importances\n",
        "\n",
        "# Show feature importance\n",
        "knn_importances = permutation_importance(knn_grid_search, X_test, y_test, random_state=12)\n",
        "\n",
        "# Create a data frame to neatly organize\n",
        "knn_importances_df = pd.DataFrame()\n",
        "knn_importances_df['Features'] = X.columns\n",
        "knn_importances_df['Mean Importance'] = knn_importances['importances_mean']\n",
        "knn_importances_df['STD Importance'] = knn_importances['importances_std']\n",
        "\n",
        "# Reorder based on the mean importance\n",
        "knn_importances_df.sort_values(by='Mean Importance', inplace=True, ascending=False)\n",
        "\n",
        "# Plot importances\n",
        "sns.barplot(data = knn_importances_df, x = 'Mean Importance', y = 'Features')\n",
        "plt.axvline(0, color = 'black', linestyle = '--')\n",
        "plt.title('KNN | Mean Importances per Features');"
      ],
      "metadata": {
        "id": "2yfItNK-rrd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Importances\n",
        "We put all the feature importance information into one dataframe and normalize it, putting it all on the same scale so it can be compared"
      ],
      "metadata": {
        "id": "EZdXxzIEr3qX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Combined Dataframe\n",
        "\n",
        "# Combine feature importances from all three models into one dataframe\n",
        "imp_compare = (\n",
        "    xgbc_importances_df[['Features', 'Mean Importance']].rename(columns={'Mean Importance': 'XGBC'})\n",
        "    .merge(lr_importances_df[['Features', 'Mean Importance']].rename(columns={'Mean Importance': 'LR'}),\n",
        "           on='Features', how='outer')\n",
        "    .merge(knn_importances_df[['Features', 'Mean Importance']].rename(columns={'Mean Importance': 'KNN'}),\n",
        "           on='Features', how='outer')\n",
        "    .fillna(0) # Replace missing values with 0\n",
        "    .set_index('Features') # Feature names as index\n",
        ")\n",
        "\n",
        "imp_compare"
      ],
      "metadata": {
        "id": "lvuut3nAqYgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Normalizing Values\n",
        "\n",
        "# Normalize feature importances so each modelâ€™s importances sum to 1\n",
        "# Making all importances positive, summing them per model, then dividing each importance by total to find proportion\n",
        "imp_norm = imp_compare.abs().div(imp_compare.abs().sum(axis=0), axis=1)\n",
        "\n",
        "# Sort features by their average importance across all models (largest first)\n",
        "imp_norm = imp_norm.loc[imp_norm.mean(axis=1).sort_values(ascending=False).index]\n",
        "imp_norm"
      ],
      "metadata": {
        "id": "h8TAEhjxqdxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Comparison Plot\n",
        "\n",
        "ax = imp_norm.plot(kind='bar', figsize=(10, 5))\n",
        "ax.set_ylabel('Normalized importance')\n",
        "ax.set_title('Feature importance comparison (normalized)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JzGMVwezrCd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Scores\n",
        "Now, we compare the precision and f-beta of each model, determining which is best\n"
      ],
      "metadata": {
        "id": "TWjZZagKr13w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores_df.set_index('Model')[['Precision Score', 'F-Beta Score']].plot(\n",
        "    kind='bar',\n",
        "    figsize=(8, 5)\n",
        ")\n",
        "\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YqH3QaeepTjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis\n",
        "We decided to also employ the use of the tabularisai/multilingual-sentiment-analysis library on HuggingFace to get an idea of what people think about stocks. We scraped many subreddits, looking for mentions of the commodities"
      ],
      "metadata": {
        "id": "DwQetmfBtCDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sentiment Analyzer Initialization\n",
        "\n",
        "# Initialize model\n",
        "model_name = 'tabularisai/multilingual-sentiment-analysis'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Function from \"how to\" section on huggingface page\n",
        "\n",
        "# Predict sentiment labels for a list of texts\n",
        "def predict_sentiment(texts):\n",
        "    # Convert text into tensors\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # Run model without tracking gradients\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Convert model outputs to probabilities\n",
        "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "    # Map numeric labels to sentiment names\n",
        "    sentiment_map = {\n",
        "        0: 'Very Negative',\n",
        "        1: 'Negative',\n",
        "        2: 'Neutral',\n",
        "        3: 'Positive',\n",
        "        4: 'Very Positive'\n",
        "    }\n",
        "\n",
        "    # Return the sentiment with the highest probability\n",
        "    return [\n",
        "        sentiment_map[p]\n",
        "        for p in torch.argmax(probabilities, dim=-1).tolist()\n",
        "    ]"
      ],
      "metadata": {
        "id": "9WOL_EjNgQdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of subreddits to scrape\n",
        "subredditlist = [\n",
        "    'news', 'worldnews', 'breakingnews', 'globalnews',\n",
        "    'wallstreetbets', 'stockmarket', 'stocks',\n",
        "    'trading', 'daytrading', 'economics', 'economy',\n",
        "]"
      ],
      "metadata": {
        "id": "e73MkrWhxsuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of commodities to look for in posts\n",
        "commodity_names = ['gold', 'silver', 'platinum', 'copper', 'oil', 'gas', 'wheat', 'soybean', 'corn']"
      ],
      "metadata": {
        "id": "RGxx4nrHxjmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get sentiment for a single piece of text\n",
        "def predict_sentiment_one(text):\n",
        "    if not isinstance(text, str) or text.strip() == '':\n",
        "        return None\n",
        "    return predict_sentiment([text])[0]\n",
        "\n",
        "# Find which commodities appear in a text\n",
        "def find_commodities(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    words = text.lower().split()\n",
        "    found = []\n",
        "    for c in commodity_names:\n",
        "        if c in words:\n",
        "            found.append(c)\n",
        "    return found"
      ],
      "metadata": {
        "id": "1WkfK3D18VNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty list to append to\n",
        "all_rows = []\n",
        "\n",
        "# Loop through each subreddit\n",
        "for item in subredditlist:\n",
        "    # Connect to Reddit\n",
        "    subscraper = praw.Reddit(\n",
        "        client_id='xdpuZuV2hoBM9kQDBhNKNQ',\n",
        "        client_secret='sSoQR5rsBFHvH6XtMwN7Oy0xSJiZRw',\n",
        "        user_agent='sub-activity-check by u/Flaky-Pattern4439',\n",
        "        check_for_async=False\n",
        "    )\n",
        "    sub = subscraper.subreddit(item)\n",
        "\n",
        "    # Get top posts from the past month\n",
        "    posts = list(sub.top(time_filter='month', limit=500))\n",
        "\n",
        "    # Loop through posts\n",
        "    for post in posts:\n",
        "        title = post.title\n",
        "        body = post.selftext\n",
        "\n",
        "        # Combine title and body for commodity detection\n",
        "        text = f'{title} {body}'\n",
        "        mentioned = find_commodities(text)\n",
        "\n",
        "        # Skip posts with no commodities\n",
        "        if not mentioned:\n",
        "            continue\n",
        "\n",
        "        # Get sentiment of title and body\n",
        "        title_sent = predict_sentiment_one(title)\n",
        "        body_sent = predict_sentiment_one(body)\n",
        "\n",
        "        # Save post information\n",
        "        all_rows.append({\n",
        "            'Subreddit': item,\n",
        "            'Commodities mentioned': mentioned,\n",
        "            'Title': title,\n",
        "            'Body': body,\n",
        "            'Title Sentiment': title_sent,\n",
        "            'Body Sentiment': body_sent\n",
        "        })\n",
        "\n",
        "# Create dataframe of all collected posts\n",
        "posts_df = pd.DataFrame(\n",
        "    all_rows,\n",
        "    columns=[\n",
        "        'Subreddit',\n",
        "        'Commodities mentioned',\n",
        "        'Title',\n",
        "        'Body',\n",
        "        'Title Sentiment',\n",
        "        'Body Sentiment'\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Count how many times each commodity is mentioned\n",
        "commodity_counts_df = (\n",
        "    posts_df\n",
        "    .explode('Commodities mentioned')           # one row per commodity mention\n",
        "    .groupby('Commodities mentioned', as_index=False)\n",
        "    .size()                                     # count mentions\n",
        "    .rename(columns={\n",
        "        'Commodities mentioned': 'Commodity',\n",
        "        'size': 'Number of mentions'\n",
        "    })\n",
        "    .sort_values('Number of mentions', ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n"
      ],
      "metadata": {
        "id": "lidhLyee1AIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Commodity Mentions\n",
        "\n",
        "commodity_counts_df"
      ],
      "metadata": {
        "id": "H4mJUy6kx0Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commodity_counts_df.set_index('Commodity')['Number of mentions'].plot(\n",
        "    kind='bar',\n",
        "    figsize=(10, 5)\n",
        ")\n",
        "plt.title('Commodity Mentions')\n",
        "plt.ylabel('Number of Mentions')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RDFCvBfp4CCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Post Sentiment Information\n",
        "\n",
        "posts_df"
      ],
      "metadata": {
        "id": "b7aETs7w1BAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Counting Unique Sentiments for Each Commodity in Titles and Bodies\n",
        "\n",
        "title_counts = (\n",
        "    posts_df\n",
        "    .explode('Commodities mentioned')\n",
        "    .groupby(['Commodities mentioned', 'Title Sentiment'])\n",
        "    .size()\n",
        "    .unstack(fill_value=0) # Missing values assigned 0\n",
        ")\n",
        "\n",
        "body_counts = (\n",
        "    posts_df\n",
        "    .explode('Commodities mentioned')\n",
        "    .groupby(['Commodities mentioned', 'Body Sentiment'])\n",
        "    .size()\n",
        "    .unstack(fill_value=0) # Missing values assigned 0\n",
        ")\n"
      ],
      "metadata": {
        "id": "Dhu0A3z112fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Commodity Sentiments in Titles\n",
        "\n",
        "title_counts"
      ],
      "metadata": {
        "id": "ZGGNXui4yYIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_counts.plot(\n",
        "    kind='bar',\n",
        "    figsize=(10, 5)\n",
        ")\n",
        "plt.title('Commodity Sentiment Counts (Titles)')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WKe-If9r4Jgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Commodity Sentiments in Bodies\n",
        "\n",
        "body_counts"
      ],
      "metadata": {
        "id": "NEbwHv5q13xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "body_counts.plot(\n",
        "    kind='bar',\n",
        "    figsize=(10, 5)\n",
        ")\n",
        "plt.title('Commodity Sentiment Counts (Bodies)')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3u1NcIZW4OPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_counts = title_counts.add(body_counts, fill_value=0).astype(int)"
      ],
      "metadata": {
        "id": "-OjwZmV016uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Commodity Sentiments in Titles and Bodies Combined\n",
        "\n",
        "total_counts"
      ],
      "metadata": {
        "id": "tO5Ioh4-1420"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_counts.plot(\n",
        "    kind='bar',\n",
        "    figsize=(10, 5)\n",
        ")\n",
        "plt.title('Commodity Sentiment Counts (Total)')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WgcZds7V4YCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "VyZsRTQS4ZqK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}